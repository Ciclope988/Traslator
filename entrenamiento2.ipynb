{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004250fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation: Sinónimos con spaCy (sin paraphrasing)\n",
    "\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Cargar modelo de spaCy para español e inglés\n",
    "nlp_es = spacy.load('es_core_news_sm')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def synonym_augmentation(text, lang='en'):\n",
    "    nlp = nlp_en if lang == 'en' else nlp_es\n",
    "    doc = nlp(text)\n",
    "    augmented = []\n",
    "    for token in doc:\n",
    "       \n",
    "        if token.pos_ in ['NOUN', 'VERB']:\n",
    "            augmented.append(token.text)\n",
    "        else:\n",
    "            augmented.append(token.text)\n",
    "    return ' '.join(augmented)\n",
    "\n",
    "# Ejemplo de uso profesional\n",
    "sample_text_en = \"This is a sample sentence for data augmentation.\"\n",
    "sample_text_es = \"Esta es una frase de ejemplo para aumentar datos.\"\n",
    "print('Original EN:', sample_text_en)\n",
    "print('Sinónimos EN:', synonym_augmentation(sample_text_en, lang='en'))\n",
    "print('Original ES:', sample_text_es)\n",
    "print('Sinónimos ES:', synonym_augmentation(sample_text_es, lang='es'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MODELO TRANSFORMER\n",
    "\n",
    "# Codificación posicional para secuencias, fundamental en arquitecturas Transformer.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)  # Buffer para asegurar que se mueve con el modelo entre CPU/GPU.\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Definición del modelo Transformer para traducción automática.\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, nhead: int, num_encoder_layers: int, num_decoder_layers: int, dim_feedforward: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.embedding_dropout = nn.Dropout(p=dropout)\n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers, norm=nn.LayerNorm(d_model))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers, norm=nn.LayerNorm(d_model))\n",
    "        self.output_norm = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Inicialización de pesos para mejorar la estabilidad y convergencia del entrenamiento.\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=self.d_model ** -0.5)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz: int, device: torch.device) -> torch.Tensor:\n",
    "        # Máscara para evitar que el modelo atienda a posiciones futuras en la secuencia objetivo.\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_padding_mask: torch.Tensor, tgt_padding_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Embedding y normalización de la secuencia fuente.\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.embedding_dropout(src)\n",
    "        src = self.input_norm(src)\n",
    "        src = self.pos_encoder(src.transpose(0, 1)).transpose(0, 1)\n",
    "        memory = self.transformer_encoder(src, src_key_padding_mask=src_padding_mask)\n",
    "        # Embedding y normalización de la secuencia objetivo.\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding_dropout(tgt)\n",
    "        tgt = self.input_norm(tgt)\n",
    "        tgt = self.pos_encoder(tgt.transpose(0, 1)).transpose(0, 1)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=src_padding_mask)\n",
    "        output = self.output_norm(output)\n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CONFIGURACIÓN Y PREPROCESAMIENTO DE DATOS (CHUNKED TRAINING)\n",
    "\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        return device\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = setup_device()\n",
    "\n",
    "PAD_IDX, SOS_IDX, EOS_IDX = 256, 257, 258\n",
    "# Vocabulario byte-level: 256 bytes + 3 tokens especiales (PAD, SOS, EOS)\n",
    "VOCAB_SIZE_BYTES = 259 + 3\n",
    "\n",
    "D_MODEL = 512\n",
    "NHEAD = 8\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "DIM_FEEDFORWARD = 2048\n",
    "DROPOUT = 0.1\n",
    "\n",
    "EPOCHS_PER_CHUNK = 10\n",
    "BATCH_SIZE = 128\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 0.0005\n",
    "WARMUP_STEPS = 500\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "USE_MIXED_PRECISION = torch.cuda.is_available()\n",
    "\n",
    "def encode_bytes(text):\n",
    "    byte_seq = list(text.encode('utf-8'))[:128]\n",
    "    return [SOS_IDX] + byte_seq + [EOS_IDX]\n",
    "\n",
    "#Chunked data \n",
    "from datasets import load_dataset\n",
    "\n",
    "chunk_slices = [\n",
    "    \"train[:20%]\",\n",
    "    \"train[20%:40%]\",\n",
    "    \"train[40%:60%]\",\n",
    "    \"train[60%:80%]\",\n",
    "    \"train[80%:100%]\"\n",
    "]\n",
    "\n",
    "def augment_translation_pair_full(pair, lang='en'):\n",
    "    src, tgt = pair['en'], pair['es']\n",
    "    src_syn = synonym_augmentation(src, lang='en')\n",
    "    tgt_syn = synonym_augmentation(tgt, lang='es')\n",
    "    return [\n",
    "        {'en': src, 'es': tgt},\n",
    "        {'en': src_syn, 'es': tgt},\n",
    "        {'en': src, 'es': tgt_syn},\n",
    "        {'en': src_syn, 'es': tgt_syn}\n",
    "    ]\n",
    "\n",
    "def prepare_chunk_data(raw_datasets):\n",
    "    augmented_examples = []\n",
    "    for item in raw_datasets:\n",
    "        augmented_examples.extend(augment_translation_pair_full(item['translation']))\n",
    "    bidirectional_examples = []\n",
    "    for ex in augmented_examples:\n",
    "        bidirectional_examples.append({'src_bytes': encode_bytes(ex['en']), 'tgt_bytes': encode_bytes(ex['es'])})\n",
    "        bidirectional_examples.append({'src_bytes': encode_bytes(ex['es']), 'tgt_bytes': encode_bytes(ex['en'])})\n",
    "    import random\n",
    "    random.shuffle(bidirectional_examples)\n",
    "    split_idx = int(len(bidirectional_examples) * 0.9)\n",
    "    train_data = bidirectional_examples[:split_idx]\n",
    "    val_data = bidirectional_examples[split_idx:]\n",
    "    return train_data, val_data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for item in batch:\n",
    "        src_batch.append(torch.tensor(item['src_bytes']))\n",
    "        tgt_batch.append(torch.tensor(item['tgt_bytes']))\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "    return src_padded, tgt_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización del modelo y optimizador antes de cargar el checkpoint\n",
    "VOCAB_SIZE = VOCAB_SIZE_BYTES if 'VOCAB_SIZE_BYTES' in globals() else 15000 + 50\n",
    "D_MODEL = D_MODEL if 'D_MODEL' in globals() else 512\n",
    "NHEAD = NHEAD if 'NHEAD' in globals() else 8\n",
    "NUM_ENCODER_LAYERS = NUM_ENCODER_LAYERS if 'NUM_ENCODER_LAYERS' in globals() else 6\n",
    "NUM_DECODER_LAYERS = NUM_DECODER_LAYERS if 'NUM_DECODER_LAYERS' in globals() else 6\n",
    "DIM_FEEDFORWARD = DIM_FEEDFORWARD if 'DIM_FEEDFORWARD' in globals() else 2048\n",
    "DROPOUT = DROPOUT if 'DROPOUT' in globals() else 0.1\n",
    "LEARNING_RATE = LEARNING_RATE if 'LEARNING_RATE' in globals() else 0.0005\n",
    "PAD_IDX = PAD_IDX if 'PAD_IDX' in globals() else 256\n",
    "import torch.nn as nn\n",
    "model = TransformerModel(VOCAB_SIZE, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716dfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación profesional de los chunks para entrenamiento con opus100 (en-es)\n",
    "from datasets import load_dataset\n",
    "\n",
    "opus100_chunk_slices = [\n",
    "    \"train[:20%]\",\n",
    "    \"train[20%:40%]\",\n",
    "    \"train[40%:60%]\",\n",
    "    \"train[60%:80%]\",\n",
    "    \"train[80%:100%]\"\n",
    "    ]\n",
    "\n",
    "def augment_translation_pair_full_opus100(pair):\n",
    "    # opus100 tiene formato {'translation': {'en': ..., 'es': ...}}\n",
    "    trans = pair.get('translation')\n",
    "    if trans is None:\n",
    "        raise KeyError(f\"No se encontró la clave 'translation' en el par: {pair}\")\n",
    "    src = trans.get('en')\n",
    "    tgt = trans.get('es')\n",
    "    if src is None or tgt is None:\n",
    "        raise KeyError(f\"No se encontraron las claves 'en' y 'es' en el par: {pair}\")\n",
    "    src_syn = synonym_augmentation(src, lang='en')\n",
    "    tgt_syn = synonym_augmentation(tgt, lang='es')\n",
    "    return [\n",
    "        {'en': src, 'es': tgt},\n",
    "        {'en': src_syn, 'es': tgt},\n",
    "        {'en': src, 'es': tgt_syn},\n",
    "        {'en': src_syn, 'es': tgt_syn}\n",
    "    ]\n",
    "\n",
    "def prepare_chunk_data_opus100(raw_datasets):\n",
    "    augmented_examples = []\n",
    "    for item in raw_datasets:\n",
    "        augmented_examples.extend(augment_translation_pair_full_opus100(item))\n",
    "    bidirectional_examples = []\n",
    "    for ex in augmented_examples:\n",
    "        bidirectional_examples.append({'src_bytes': encode_bytes(ex['en']), 'tgt_bytes': encode_bytes(ex['es'])})\n",
    "        bidirectional_examples.append({'src_bytes': encode_bytes(ex['es']), 'tgt_bytes': encode_bytes(ex['en'])})\n",
    "    import random\n",
    "    random.shuffle(bidirectional_examples)\n",
    "    split_idx = int(len(bidirectional_examples) * 0.9)\n",
    "    train_data = bidirectional_examples[:split_idx]\n",
    "    val_data = bidirectional_examples[split_idx:]\n",
    "    return train_data, val_data\n",
    "\n",
    "opus100_dataloaders = []\n",
    "for chunk_idx, chunk_slice in enumerate(opus100_chunk_slices):\n",
    "    print(f\"\\n=== Preparando chunk {chunk_idx+1}/{len(opus100_chunk_slices)}: {chunk_slice} ===\")\n",
    "    raw_datasets = load_dataset(\"opus100\", \"en-es\", split=chunk_slice)\n",
    "    train_data, val_data = prepare_chunk_data_opus100(raw_datasets)\n",
    "    train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    val_dl = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    opus100_dataloaders.append((train_dl, val_dl))\n",
    "    print(f\"  - Ejemplos de entrenamiento: {len(train_data)}\")\n",
    "    print(f\"  - Ejemplos de validación: {len(val_data)}\")\n",
    "    print(f\"  - Batches de entrenamiento: {len(train_dl)}\")\n",
    "    print(f\"  - Batches de validación: {len(val_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19385761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning incremental con los chunks de opus100, comenzando desde best_model_doctrans_chunk5.pt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Cargar pesos iniciales desde best_model_doctrans_chunk5.pt\n",
    "init_ckpt_path = r'C:\\Users\\Luis\\Downloads\\entrenamiento\\best_model_doctrans_chunk5.pt'\n",
    "if os.path.exists(init_ckpt_path):\n",
    "    checkpoint = torch.load(init_ckpt_path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Pesos iniciales cargados desde {init_ckpt_path}.\")\n",
    "else:\n",
    "    print(f\"No se encontró {init_ckpt_path}, se inicia con pesos actuales.\")\n",
    "\n",
    "EPOCHS_PER_CHUNK = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_losses_por_chunk = []\n",
    "last_train_losses_por_chunk = []\n",
    "\n",
    "for chunk_idx, (train_dl, val_dl) in enumerate(opus100_dataloaders):\n",
    "    # Si no es el primer chunk, carga el checkpoint del chunk anterior\n",
    "    if chunk_idx > 0:\n",
    "        prev_ckpt_path = f'best_model_opus100_chunk{chunk_idx}.pt'\n",
    "        if os.path.exists(prev_ckpt_path):\n",
    "            checkpoint = torch.load(prev_ckpt_path, map_location=DEVICE)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f\"Pesos cargados desde {prev_ckpt_path} para chunk {chunk_idx+1}.\")\n",
    "        else:\n",
    "            print(f\"No se encontró {prev_ckpt_path}, se continúa con los pesos actuales.\")\n",
    "\n",
    "    print(f\"\\n=== Fine-tuning en chunk {chunk_idx+1}/{len(opus100_dataloaders)} ===\")\n",
    "    total_steps = len(train_dl) * EPOCHS_PER_CHUNK\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps, pct_start=0.1, anneal_strategy='cos')\n",
    "    best_val_loss = float('inf')\n",
    "    last_train_loss = None\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    for epoch in range(1, EPOCHS_PER_CHUNK + 1):\n",
    "        print(f\"\\n--- Epoch {epoch}/{EPOCHS_PER_CHUNK} ---\")\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, optimizer, nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1), train_dl, scheduler, scaler)\n",
    "        val_loss = evaluate(model, nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1), val_dl)\n",
    "        elapsed = time.time() - start_time\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        last_train_loss = train_loss\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "        print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        print(f\"Epoch time: {elapsed:.2f} seconds\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, f'best_model_opus100_chunk{chunk_idx+1}.pt')\n",
    "            print(f\"Model saved (val_loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping activated at epoch {epoch}\")\n",
    "                break\n",
    "    best_val_losses_por_chunk.append(best_val_loss)\n",
    "    last_train_losses_por_chunk.append(last_train_loss)\n",
    "print(\"Fine-tuning incremental completado con opus100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b00e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning secuencial: opus100 -> NickyNicky/Colossal\n",
    "import os\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_with_dataloaders(dataloaders, checkpoint_prefix, initial_ckpt_path=None):\n",
    "    global model, optimizer\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_losses_por_chunk = []\n",
    "    last_train_losses_por_chunk = []\n",
    "    # Cargar pesos iniciales si se especifica\n",
    "    if initial_ckpt_path is not None and os.path.exists(initial_ckpt_path):\n",
    "        checkpoint = torch.load(initial_ckpt_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Pesos iniciales cargados desde {initial_ckpt_path}.\")\n",
    "    elif initial_ckpt_path is not None:\n",
    "        print(f\"No se encontró {initial_ckpt_path}, se inicia con pesos actuales.\")\n",
    "    for chunk_idx, (train_dl, val_dl) in enumerate(dataloaders):\n",
    "        # Si no es el primer chunk, carga el checkpoint del chunk anterior\n",
    "        if chunk_idx > 0:\n",
    "            prev_ckpt_path = f'{checkpoint_prefix}_chunk{chunk_idx}.pt'\n",
    "            if os.path.exists(prev_ckpt_path):\n",
    "                checkpoint = torch.load(prev_ckpt_path, map_location=DEVICE)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                print(f\"Pesos cargados desde {prev_ckpt_path} para chunk {chunk_idx+1}.\")\n",
    "            else:\n",
    "                print(f\"No se encontró {prev_ckpt_path}, se continúa con los pesos actuales.\")\n",
    "\n",
    "        print(f\"\\n=== Fine-tuning en chunk {chunk_idx+1}/{len(dataloaders)} ===\")\n",
    "        total_steps = len(train_dl) * EPOCHS_PER_CHUNK\n",
    "        scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=total_steps, pct_start=0.1, anneal_strategy='cos')\n",
    "        best_val_loss = float('inf')\n",
    "        last_train_loss = None\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        for epoch in range(1, EPOCHS_PER_CHUNK + 1):\n",
    "            print(f\"\\n--- Epoch {epoch}/{EPOCHS_PER_CHUNK} ---\")\n",
    "            start_time = time.time()\n",
    "            train_loss = train_epoch(model, optimizer, nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1), train_dl, scheduler, scaler)\n",
    "            val_loss = evaluate(model, nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1), val_dl)\n",
    "            elapsed = time.time() - start_time\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            last_train_loss = train_loss\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation loss: {val_loss:.4f}\")\n",
    "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "            print(f\"Epoch time: {elapsed:.2f} seconds\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }, f'{checkpoint_prefix}_chunk{chunk_idx+1}.pt')\n",
    "                print(f\"Model saved (val_loss: {val_loss:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"Patience: {patience_counter}/{patience}\")\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping activated at epoch {epoch}\")\n",
    "                    break\n",
    "        best_val_losses_por_chunk.append(best_val_loss)\n",
    "        last_train_losses_por_chunk.append(last_train_loss)\n",
    "    print(f\"Fine-tuning incremental completado con {checkpoint_prefix}\")\n",
    "    print(f\"Mejor validation loss final: {best_val_loss:.4f}\")\n",
    "    # Cargar el último checkpoint\n",
    "    last_ckpt_path = f'{checkpoint_prefix}_chunk{len(dataloaders)}.pt'\n",
    "    if os.path.exists(last_ckpt_path):\n",
    "        checkpoint = torch.load(last_ckpt_path, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Pesos cargados desde {last_ckpt_path} para continuar el siguiente entrenamiento.\")\n",
    "    else:\n",
    "        print(f\"No se encontró {last_ckpt_path}, se continúa con los pesos actuales.\")\n",
    "\n",
    "# --- Entrenamiento secuencial ---\n",
    "print(\"Entrenando con opus100...\")\n",
    "train_with_dataloaders(opus100_dataloaders, 'best_model_opus100', initial_ckpt_path=r'C:\\Users\\Luis\\Downloads\\entrenamiento\\best_model_doctrans_chunk5.pt')\n",
    "\n",
    "# Preparar dataloaders para NickyNicky/Colossal\n",
    "colossal_chunk_slices = [\n",
    "    \"train[:20%]\",\n",
    "    \"train[20%:40%]\",\n",
    "    \"train[40%:60%]\",\n",
    "    \"train[60%:80%]\",\n",
    "    \"train[80%:100%]\"\n",
    "]\n",
    "def augment_translation_pair_full_colossal(pair):\n",
    "    src = pair.get('english') or pair.get('en') or pair.get('source')\n",
    "    tgt = pair.get('spanish') or pair.get('es') or pair.get('target')\n",
    "    if src is None or tgt is None:\n",
    "        if pair.get('task') == 'en_es' and pair.get('prompt') and pair.get('chosen'):\n",
    "            src = pair['prompt']\n",
    "            tgt = pair['chosen']\n",
    "        elif pair.get('task') == 'es_en' and pair.get('prompt') and pair.get('chosen'):\n",
    "            src = pair['chosen']\n",
    "            tgt = pair['prompt']\n",
    "        else:\n",
    "            raise KeyError(f\"No se encontraron las claves correctas en el par: {pair}\")\n",
    "    src_syn = synonym_augmentation(src, lang='en')\n",
    "    tgt_syn = synonym_augmentation(tgt, lang='es')\n",
    "    return [\n",
    "        {'en': src, 'es': tgt},\n",
    "        {'en': src_syn, 'es': tgt},\n",
    "        {'en': src, 'es': tgt_syn},\n",
    "        {'en': src_syn, 'es': tgt_syn}\n",
    "    ]\n",
    "def prepare_chunk_data_colossal(raw_datasets):\n",
    "    augmented_examples = []\n",
    "    for item in raw_datasets:\n",
    "        augmented_examples.extend(augment_translation_pair_full_colossal(item))\n",
    "    bidirectional_examples = []\n",
    "    for ex in augmented_examples:\n",
    "        bidirectional_examples.append({'src_bytes': encode_bytes(ex['en']), 'tgt_bytes': encode_bytes(ex['es'])})\n",
    "        bidirectional_examples.append({'src_bytes': encode_bytes(ex['es']), 'tgt_bytes': encode_bytes(ex['en'])})\n",
    "    import random\n",
    "    random.shuffle(bidirectional_examples)\n",
    "    split_idx = int(len(bidirectional_examples) * 0.9)\n",
    "    train_data = bidirectional_examples[:split_idx]\n",
    "    val_data = bidirectional_examples[split_idx:]\n",
    "    return train_data, val_data\n",
    "colossal_dataloaders = []\n",
    "for chunk_idx, chunk_slice in enumerate(colossal_chunk_slices):\n",
    "    print(f\"\\n=== Preparando chunk {chunk_idx+1}/{len(colossal_chunk_slices)}: {chunk_slice} ===\")\n",
    "    raw_datasets = load_dataset(\"NickyNicky/Colossal_Translation_Spanish_to_English_AND_English_to_Spanish_ORPO_DPO_Gemma\", split=chunk_slice)\n",
    "    train_data, val_data = prepare_chunk_data_colossal(raw_datasets)\n",
    "    train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    val_dl = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    colossal_dataloaders.append((train_dl, val_dl))\n",
    "    print(f\"  - Ejemplos de entrenamiento: {len(train_data)}\")\n",
    "    print(f\"  - Ejemplos de validación: {len(val_data)}\")\n",
    "    print(f\"  - Batches de entrenamiento: {len(train_dl)}\")\n",
    "    print(f\"  - Batches de validación: {len(val_dl)}\")\n",
    "\n",
    "print(\"Entrenando con NickyNicky/Colossal...\")\n",
    "train_with_dataloaders(colossal_dataloaders, 'best_model_colossal', initial_ckpt_path=f'best_model_opus100_chunk{len(opus100_dataloaders)}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2e979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
